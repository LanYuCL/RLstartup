{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN-GPU.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMu9Kgfcrpb9AthsaoaKk7R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LanYuCL/RLstartup/blob/master/DQN_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybKekmgKWOk1",
        "outputId": "dbbf55a0-2ef4-4090-b63c-ab72747e24f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WURKwpqW3C5",
        "outputId": "2d717c8e-5a93-46a1-8bc2-fb5197f9960b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pip install pygame"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pygame in /usr/local/lib/python3.6/dist-packages (2.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XDvssQd64Pf"
      },
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5esgX013vPe"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1_I0GqmYJkX",
        "outputId": "6f3d0aff-5a52-4fc9-c408-74edc500021e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pygame\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pickle\n",
        "import random\n",
        "import tqdm\n",
        "import sys\n",
        "import math\n",
        "import numpy as np\n",
        "from abc import ABC\n",
        "from PIL import Image\n",
        "from collections import deque, OrderedDict\n",
        "from pygame.locals import *\n",
        "from pyvirtualdisplay import Display\n",
        "thedisplay = Display(visible=0, size=(800, 1200))\n",
        "thedisplay.start()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pygame 2.0.0 (SDL 2.0.12, python 3.6.9)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f9db9cbbcf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoQMQsarX-nG"
      },
      "source": [
        "# ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# Project: Reinforcement Learning on Snake\n",
        "# Author: Cai Ruikai\n",
        "# Date: 2020.10.10\n",
        "# ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "default_config = {\n",
        "    'SCREEN_WIDTH': 200,  # 屏幕宽度\n",
        "    'SCREEN_HEIGHT': 200,  # 屏幕高度\n",
        "    'BLOCK_SIZE': 20,  # 方格大小\n",
        "\n",
        "    'CROSS_BOUNDARY': True,  # 是否允许穿过边界\n",
        "\n",
        "    'BACKGROUND_COLOR': (60, 60, 60),  # 背景颜色\n",
        "    'SNAKE_COLOR': (0, 160, 100),  # 蛇的颜色\n",
        "    'FOOD_COLOR': (240, 240, 240),  # 水果的颜色\n",
        "    'OBSTACLE_COLOR': (255, 0, 0),  # 障碍物颜色\n",
        "\n",
        "    'OBSTACLE': False,\n",
        "    'OBSTACLE_NUM': 2,  # 障碍物数量\n",
        "    'OBSTACLE_FRESH': False,  # 是否刷新障碍物位置\n",
        "    'OBSTACLE_FRESH_RATE': 30  # 障碍物刷新频率\n",
        "}\n",
        "\n",
        "\n",
        "class Snake_Env:\n",
        "    def __init__(self, config=None):\n",
        "        # use default config\n",
        "        if not config:\n",
        "            config = default_config\n",
        "\n",
        "        # game environment setting\n",
        "        self._SCREEN_WIDTH = config['SCREEN_WIDTH']\n",
        "        self._SCREEN_HEIGHT = config['SCREEN_HEIGHT']\n",
        "        self._BLOCK_SIZE = config['BLOCK_SIZE']\n",
        "        self._X_AREA = (0, self._SCREEN_WIDTH // self._BLOCK_SIZE - 1)\n",
        "        self._Y_AREA = (0, self._SCREEN_HEIGHT // self._BLOCK_SIZE - 1)\n",
        "\n",
        "        self._CROSS_BOUNDARY = config['CROSS_BOUNDARY']\n",
        "\n",
        "        self._OBSTACLE_NUM = config['OBSTACLE_NUM']\n",
        "        self._OBSTACLE_FRESH = config['OBSTACLE_FRESH']\n",
        "        self._OBSTACLE_FRESH_RATE = config['OBSTACLE_FRESH_RATE']\n",
        "\n",
        "        self._BACKGROUND_COLOR = config['BACKGROUND_COLOR']\n",
        "        self._SNAKE_COLOR = config['SNAKE_COLOR']\n",
        "        self._OBSTACLE_COLOR = config['OBSTACLE_COLOR']\n",
        "        self._FOOD_COLOR = config['FOOD_COLOR']\n",
        "        self._OBSTACLE=config['OBSTACLE']\n",
        "\n",
        "        # game status init\n",
        "        self._step = 0\n",
        "        self._score = 0\n",
        "        self._step_reward = 0\n",
        "        self._game_over = 0\n",
        "\n",
        "        # pygame init\n",
        "        pygame.init()\n",
        "        self.screen = pygame.display.set_mode((self._SCREEN_WIDTH, self._SCREEN_HEIGHT))\n",
        "        pygame.display.set_caption('Snake     Score:0')\n",
        "\n",
        "        # snake,fruits,obstacles init\n",
        "        self._snake, self._death_pos, self._foods, self._obstacles = deque(), (-1, 0), list(), list()\n",
        "        self._init_spfo()\n",
        "        self.render()\n",
        "        pygame.display.update()\n",
        "\n",
        "        # obs_dim, act_dim\n",
        "        self._obs_dim = (self._SCREEN_WIDTH, self._SCREEN_HEIGHT, 3)\n",
        "        self._act_dim = 4\n",
        "\n",
        "    def _init_spfo(self):\n",
        "        # snake\n",
        "        for i in range(3):\n",
        "            self._snake.append((3 - i, 0))\n",
        "        # pos\n",
        "        self._death_pos = (-1, 0)\n",
        "        # foods\n",
        "        self._foods.append(self._generate_xy())\n",
        "        # obstacles\n",
        "        if self._OBSTACLE:\n",
        "            for i in range(self._OBSTACLE_NUM):\n",
        "                self._obstacles.append(self._generate_xy())\n",
        "\n",
        "    def _generate_xy(self):\n",
        "        x = random.randint(self._X_AREA[0], self._X_AREA[1])\n",
        "        y = random.randint(self._Y_AREA[0], self._Y_AREA[1])\n",
        "        while (x, y) in set(list(self._snake) + self._obstacles + self._foods):\n",
        "            x = random.randint(self._X_AREA[0], self._X_AREA[1])\n",
        "            y = random.randint(self._Y_AREA[0], self._Y_AREA[1])\n",
        "        return x, y\n",
        "\n",
        "    def _refresh_food(self, eaten_food):\n",
        "        if eaten_food == 0:\n",
        "            self._foods[0] = self._generate_xy()\n",
        "        # if eaten_food == 1:\n",
        "        #     self._foods.pop()\n",
        "        # if self._step % self._X_AREA[1]*2 == 0:\n",
        "        #     self._foods = self._foods[:1]\n",
        "        #     self._foods.append(self._generate_xy())\n",
        "        # if self._step % self._X_AREA[1] > self._X_AREA[1] *0.9:\n",
        "        #     self._foods = self._foods[:1]\n",
        "\n",
        "    def _refresh_obstacle(self):\n",
        "        if not self._OBSTACLE: return\n",
        "        if self._step % self._OBSTACLE_FRESH_RATE == 0:\n",
        "            self._obstacles = list()\n",
        "            for i in range(self._OBSTACLE_NUM):\n",
        "                self._obstacles.append(self._generate_xy())\n",
        "\n",
        "    def _print_over(self):\n",
        "        font = pygame.font.Font(None, 50)\n",
        "        fwidth, fheight = font.size('GAME OVER')\n",
        "        imgtext = font.render('GAME OVER', True, (255, 0, 0))\n",
        "        self.screen.blit(imgtext, (self._SCREEN_WIDTH // 2 - fwidth // 2, self._SCREEN_HEIGHT // 2 - fheight // 2))\n",
        "\n",
        "    def _move_snake(self, action):\n",
        "        env_action = ((-1, 0), (1, 0), (0, -1), (0, 1))\n",
        "        reward, eaten_food = -0.15, -1\n",
        "        # judge whether action is legal\n",
        "        contrary = {(0, 1): (0, -1), (0, -1): (0, 1), (1, 0): (-1, 0), (-1, 0): (1, 0)}\n",
        "        action = env_action[action]\n",
        "        if action == self._death_pos:\n",
        "            self._game_over = 1\n",
        "            # print('\\nDeath: action is legal')\n",
        "        else:\n",
        "            next_head = (self._snake[0][0] + action[0], self._snake[0][1] + action[1])\n",
        "            # obstacles\n",
        "            if next_head in self._obstacles:\n",
        "                self._game_over = 1\n",
        "                # print('\\nDeath: obstacles')\n",
        "            # boundary\n",
        "            if self._CROSS_BOUNDARY:\n",
        "                if next_head[0] < 0:\n",
        "                    next_head = (self._X_AREA[1], next_head[1])\n",
        "                elif next_head[0] > self._X_AREA[1]:\n",
        "                    next_head = (0, next_head[1])\n",
        "                elif next_head[1] < 0:\n",
        "                    next_head = (next_head[0], self._Y_AREA[1])\n",
        "                elif next_head[1] > self._Y_AREA[1]:\n",
        "                    next_head = (next_head[0], 0)\n",
        "            else:\n",
        "                if next_head[0] < 0 or next_head[0] > self._X_AREA[1] or next_head[1] < 0 or next_head[1] > \\\n",
        "                        self._Y_AREA[\n",
        "                            1]:\n",
        "                    self._game_over = 1\n",
        "                    # print('\\nDeath: boundary')\n",
        "            # body:\n",
        "            if next_head in self._snake:\n",
        "                self._game_over = 1\n",
        "                # print('\\nDeath: body')\n",
        "\n",
        "            # fruit\n",
        "            if next_head in self._foods:\n",
        "                eaten_food = self._foods.index(next_head)\n",
        "                # reward += 10 if eaten_food == 0 else 50\n",
        "                reward += 50\n",
        "\n",
        "            else:\n",
        "                self._snake.pop()\n",
        "\n",
        "            self._snake.appendleft(next_head)\n",
        "        if not self._game_over:\n",
        "            self._death_pos = contrary[action]\n",
        "            dis = (math.sqrt(pow((self._foods[0][0] - next_head[0]), 2) + pow((self._foods[0][1] - next_head[1]), 2)))\n",
        "            dis_reward = (1 / max(1.0, dis)) * 1\n",
        "            reward += dis_reward\n",
        "        else:\n",
        "            reward = -10\n",
        "        self._score += eaten_food+1 if eaten_food<1 else 5\n",
        "        return reward, eaten_food\n",
        "\n",
        "    def render(self):\n",
        "        # draw background\n",
        "        self.screen.fill(self._BACKGROUND_COLOR)\n",
        "        # draw gird and x-axis\n",
        "        for x in range(self._BLOCK_SIZE, self._SCREEN_WIDTH, self._BLOCK_SIZE):\n",
        "            pygame.draw.line(self.screen, (0, 0, 0), (x, 0), (x, self._SCREEN_HEIGHT), 1)\n",
        "        # draw gird and y-axis\n",
        "        for y in range(self._BLOCK_SIZE, self._SCREEN_HEIGHT, self._BLOCK_SIZE):\n",
        "            pygame.draw.line(self.screen, (0, 0, 0), (0, y), (self._SCREEN_WIDTH, y), 1)\n",
        "        # draw food\n",
        "        for index, food in enumerate(self._foods):\n",
        "            food_color = self._FOOD_COLOR if index == 0 else (255, 215, 0)\n",
        "            pygame.draw.circle(self.screen, food_color,\n",
        "                               (food[0] * self._BLOCK_SIZE + self._BLOCK_SIZE // 2,\n",
        "                                food[1] * self._BLOCK_SIZE + self._BLOCK_SIZE // 2),\n",
        "                               self._BLOCK_SIZE // 2, 0)\n",
        "        # draw obstacle\n",
        "        for obs in self._obstacles:\n",
        "            pygame.draw.rect(self.screen, self._OBSTACLE_COLOR,\n",
        "                             ((obs[0] * self._BLOCK_SIZE, obs[1] * self._BLOCK_SIZE),\n",
        "                              (self._BLOCK_SIZE, self._BLOCK_SIZE)),\n",
        "                             0)\n",
        "        # draw snake\n",
        "        for index, node in enumerate(self._snake):\n",
        "            if index == 0:\n",
        "                pygame.draw.circle(self.screen, self._SNAKE_COLOR,\n",
        "                                   (node[0] * self._BLOCK_SIZE + self._BLOCK_SIZE // 2,\n",
        "                                    node[1] * self._BLOCK_SIZE + self._BLOCK_SIZE // 2),\n",
        "                                   self._BLOCK_SIZE // 2, 0)\n",
        "            else:\n",
        "                pygame.draw.rect(self.screen, self._SNAKE_COLOR,\n",
        "                                 ((node[0] * self._BLOCK_SIZE, node[1] * self._BLOCK_SIZE),\n",
        "                                  (self._BLOCK_SIZE, self._BLOCK_SIZE)),\n",
        "                                 0)\n",
        "        if self._game_over:\n",
        "            self._print_over()\n",
        "\n",
        "        pygame.display.set_caption('Score:{:.3f}'.format(self._score))\n",
        "        pygame.display.update()\n",
        "\n",
        "    def init(self):\n",
        "        self.__init__()\n",
        "        obs = pygame.surfarray.array3d(pygame.display.get_surface()).transpose((1, 0, 2))\n",
        "        return obs\n",
        "\n",
        "    def obs_dim(self):\n",
        "        return self._obs_dim\n",
        "\n",
        "    def act_dim(self):\n",
        "        return self._act_dim\n",
        "\n",
        "    def reset(self):\n",
        "        return self.init()\n",
        "\n",
        "    def frame_step(self, action):\n",
        "        self._step += 1\n",
        "\n",
        "        self._step_reward, eaten_food = self._move_snake(action)\n",
        "        self._refresh_food(eaten_food)\n",
        "        if self._OBSTACLE_FRESH:\n",
        "            self._refresh_obstacle()\n",
        "\n",
        "        self.render()\n",
        "        obs = pygame.surfarray.array3d(pygame.display.get_surface()).transpose((1, 0, 2))\n",
        "\n",
        "        return obs, self._step_reward, self._score, self._game_over, self.get_game_info()\n",
        "\n",
        "    @staticmethod\n",
        "    def get_human_action():\n",
        "        action = None\n",
        "        while not action:\n",
        "            for event in pygame.event.get():\n",
        "                if event.type == QUIT:\n",
        "                    sys.exit()\n",
        "                elif event.type == pygame.KEYUP:\n",
        "                    if event.key == pygame.K_LEFT:\n",
        "                        return 0\n",
        "                    if event.key == pygame.K_RIGHT:\n",
        "                        return 1\n",
        "                    if event.key == pygame.K_UP:\n",
        "                        return 2\n",
        "                    if event.key == pygame.K_DOWN:\n",
        "                        return 3\n",
        "                else:\n",
        "                    pass\n",
        "\n",
        "    def get_game_info(self):\n",
        "        info = {'step': self._step,\n",
        "                'snake': list(self._snake),\n",
        "                'obstacles': self._obstacles,\n",
        "                'foods': self._foods}\n",
        "        return info\n",
        "\n",
        "    def print_game_info(self):\n",
        "        info = self.get_game_info()\n",
        "        print('\\ncurrent step:{} reward :{} game score : {}'.format(info['step'], self._step_reward, self._score))\n",
        "        for k, v in info.items():\n",
        "            if k == 'count':\n",
        "                continue\n",
        "            print(k, v)\n",
        "\n",
        "\n",
        "def demo():\n",
        "    env = Snake_Env()\n",
        "    env.init()\n",
        "    env.render()\n",
        "    game_over = False\n",
        "    while not game_over:\n",
        "        human_action = env.get_human_action()\n",
        "        obs, step_reward, game_score, game_over, info = env.frame_step(human_action)\n",
        "        env.print_game_info()\n",
        "        if game_over:\n",
        "            print(game_score)\n",
        "            game_over = False\n",
        "            env.reset()\n",
        "            env.render()\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EtfP1q-XRxn"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def to_tensor(x):\n",
        "    \"\"\"A helper function to transform a numpy array to a Pytorch Tensor\"\"\"\n",
        "    if isinstance(x, np.ndarray):\n",
        "        x = torch.from_numpy(x).type(torch.float32).to(device)\n",
        "    assert isinstance(x, torch.Tensor), type(x)\n",
        "    if x.dim() == 3 or x.dim() == 1:\n",
        "        x = x.unsqueeze(0)\n",
        "    assert x.dim() == 2 or x.dim() == 4, x.shape\n",
        "    return x\n",
        "\n",
        "\n",
        "def process_state(state):\n",
        "    # covert env's RGB format [H,W,C] to [N,C,H,W]\n",
        "    # also covert it to Tensor\n",
        "    state = torch.from_numpy(state).type(torch.float32).to(device)\n",
        "    if state.dim() == 3:\n",
        "        state = state.permute(2, 0, 1).unsqueeze(0)\n",
        "    elif state.dim() == 4:\n",
        "        state = state.permute(0, 3, 1, 2)\n",
        "    assert state.dim() == 4, state.shape\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity=50000, learn_start=500):\n",
        "        self.capacity = capacity\n",
        "        self.learn_start = learn_start\n",
        "\n",
        "        self.memory = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, transition):\n",
        "        self.memory.append(transition)\n",
        "        if len(self.memory) == self.learn_start:\n",
        "            print(\"Current memory contains {} transitions,start learning!\".format(self.learn_start))\n",
        "\n",
        "    def get_batch(self, batch_size):\n",
        "        batch = random.sample(self.memory, batch_size)\n",
        "\n",
        "        states = np.stack([transition[0] for transition in batch])\n",
        "        next_states = np.stack([transition[3] for transition in batch])\n",
        "\n",
        "        actions = to_tensor(np.stack([transition[1] for transition in batch])).long().view(-1, 1)\n",
        "\n",
        "        rewards = to_tensor(np.stack([transition[2] for transition in batch])).squeeze()\n",
        "        not_done_mask = to_tensor(np.stack([1 - transition[4] for transition in batch])).squeeze()\n",
        "\n",
        "        return states, actions, rewards, next_states, not_done_mask\n",
        "\n",
        "    def load(self, path):\n",
        "        self.memory = pickle.load(open(path, 'rb'))\n",
        "\n",
        "    def save(self, path):\n",
        "        pickle.dump(self.memory, open(path, 'wb'))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMf3HjgNXMp0",
        "outputId": "6036011b-07bb-49c8-dd30-e89977229625",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "source": [
        "\n",
        "\n",
        "# Double DQN method\n",
        "DQN_Default_Conf = {\n",
        "    'memory_size': 100000,\n",
        "    'learn_start': 500,\n",
        "    'batch_size': 64,\n",
        "    'learn_freq': 2,\n",
        "    'target_update_freq': 30,\n",
        "    'clip_norm': 5,\n",
        "    'learning_rate': 0.001,\n",
        "    'eps': 0.3,\n",
        "    'max_train_iteration': 5000000,\n",
        "    'reward_threshold': 1000,\n",
        "    'max_episode_length': 500,\n",
        "    'gamma': 0.9,\n",
        "    'evaluate_int': 1,\n",
        "}\n",
        "\n",
        "\n",
        "class DQN_Network(nn.Module, ABC):\n",
        "    def __init__(self, obs_dim=(520, 520, 3), act_dim=4):\n",
        "        super(DQN_Network, self).__init__()\n",
        "\n",
        "        self.input_shape = obs_dim\n",
        "        self.num_actions = act_dim\n",
        "        self.model = nn.Sequential(OrderedDict([\n",
        "            ('conv1', nn.Conv2d(3, 16, 3, 2)),\n",
        "            ('relu1', nn.ReLU()),\n",
        "            ('conv2', nn.Conv2d(16, 32, 3)),\n",
        "            ('relu2', nn.ReLU()),\n",
        "            ('conv3', nn.Conv2d(32, 32, 3, 2)),\n",
        "            ('relu3', nn.ReLU()),\n",
        "            ('conv4', nn.Conv2d(32, 16, 3)),\n",
        "            ('relu4', nn.ReLU()),\n",
        "            ('conv5', nn.Conv2d(16, 1, 3, 2)),\n",
        "            ('relu5', nn.ReLU()),\n",
        "            # ('conv6', nn.Conv2d(16, 1, 3)),\n",
        "            # ('relu6', nn.ReLU()),\n",
        "            ('flatten', nn.Flatten()),\n",
        "            ('linear1', nn.Linear(484, 128)),\n",
        "            ('relu7', nn.ReLU()),\n",
        "            ('linear2', nn.Linear(128, act_dim))\n",
        "            # ('linear2', nn.Linear(512, self.num_actions))\n",
        "        ]))\n",
        "        # print('Network Build Done:\\n', self.model)\n",
        "\n",
        "    def forward(self, observation):\n",
        "        observation = process_state(observation)\n",
        "        return self.model(observation)\n",
        "\n",
        "\n",
        "class DQN_Agent(DQN_Network, ABC):\n",
        "    def __init__(self):\n",
        "        super(DQN_Agent, self).__init__()\n",
        "\n",
        "    def load_weights(self, weights=None):\n",
        "        if weights:\n",
        "            self.mode.load_state_dict(weights)\n",
        "        return self.model\n",
        "\n",
        "\n",
        "class DQN_Method:\n",
        "    def __init__(self, config=None):\n",
        "        if not config:\n",
        "            config = DQN_Default_Conf\n",
        "        # parameters\n",
        "        self.best_scores = 0\n",
        "        self.learn_freq = config[\"learn_freq\"]\n",
        "        self.learn_start = config[\"learn_start\"]\n",
        "        self.learning_rate = config[\"learning_rate\"]\n",
        "        self.target_update_freq = config[\"target_update_freq\"]\n",
        "        self.memory = ReplayMemory(capacity=config[\"memory_size\"], learn_start=config[\"learn_start\"])\n",
        "\n",
        "        self.batch_size = config[\"batch_size\"]\n",
        "        self.max_train_iteration = config[\"max_train_iteration\"]\n",
        "        self.max_episode_length = config[\"max_episode_length\"]\n",
        "\n",
        "        self.eps = config[\"eps\"]\n",
        "        self.gamma = config[\"gamma\"]\n",
        "        self.clip_norm = config[\"clip_norm\"]\n",
        "        self.evaluate_int = config[\"evaluate_int\"]\n",
        "        self.reward_threshold = config[\"reward_threshold\"]\n",
        "\n",
        "        self.total_step = 0\n",
        "        self.step_since_update = 0\n",
        "        self.step_since_evaluate = 0\n",
        "\n",
        "        # create environment\n",
        "        self.env = Snake_Env()\n",
        "        self.obs_dim = self.env.obs_dim()\n",
        "        self.act_dim = self.env.act_dim()\n",
        "\n",
        "        # create double DQN network\n",
        "        self.network = DQN_Network(self.obs_dim, self.act_dim).to(device)\n",
        "        self.network.eval()\n",
        "\n",
        "        self.target_network = DQN_Network(self.obs_dim, self.act_dim).to(device)\n",
        "        self.target_network.load_state_dict(self.network.state_dict())\n",
        "        self.network.eval()\n",
        "\n",
        "        # set optimizer and loss\n",
        "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=self.learning_rate)\n",
        "        self.loss = nn.MSELoss()\n",
        "\n",
        "    def compute_action(self, observation, eps=None):\n",
        "        values = self.target_network(observation)\n",
        "        values = values.cpu().detach().numpy()\n",
        "        if not eps:\n",
        "            eps = self.eps\n",
        "        action = np.argmax(values) if np.random.random() > eps else np.random.choice(self.act_dim)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def prepare_memory(self):\n",
        "        #pbar = tqdm.tqdm(total=self.learn_start, desc=\"preparing replay memory\")\n",
        "        while len(self.memory) < self.learn_start:\n",
        "            env = Snake_Env()\n",
        "            current_state = env.reset()\n",
        "            act = self.compute_action(current_state)\n",
        "            for t in range(self.max_episode_length):\n",
        "\n",
        "                next_state, reward, score, game_over, other_info = env.frame_step(act)\n",
        "                transition = (current_state, act, reward, next_state, game_over)\n",
        "                # self.test(transition)\n",
        "\n",
        "                self.memory.push(transition)\n",
        "                #pbar.update()\n",
        "                current_state = next_state\n",
        "                act = self.compute_action(current_state)\n",
        "                if game_over:\n",
        "                    break\n",
        "        #pbar.close()\n",
        "\n",
        "    def test(self, transition):\n",
        "        current_state, act, reward, next_state, game_over = transition\n",
        "        current_state = Image.fromarray(np.uint8(current_state))\n",
        "        current_state.show(title=\"current_state\")\n",
        "        next_state = Image.fromarray(np.uint8(next_state))\n",
        "        next_state.show(title=\"next_state\")\n",
        "        print(act, reward, game_over)\n",
        "        input()\n",
        "\n",
        "    def train(self,  pre_trained=None):\n",
        "        if pre_trained:\n",
        "            self.network=torch.load(pre_trained)\n",
        "            self.target_network=torch.load(pre_trained)\n",
        "            self.best_scores=float(pre_trained.split('_')[1])\n",
        "            self.total_step=int(pre_trained.split('_')[2][:-3])\n",
        "\n",
        "        self.prepare_memory()\n",
        "        print('Start Training')\n",
        "        # train network in max_train_iteration\n",
        "        for train_iteration in range(self.max_train_iteration):\n",
        "            current_state = self.env.reset()\n",
        "            act = self.compute_action(current_state)\n",
        "            stat = {\"loss\": []}\n",
        "\n",
        "            # each train iteration has max episode length\n",
        "            for t in range(self.max_episode_length):\n",
        "\n",
        "                next_state, reward, score, game_over, other_info = self.env.frame_step(act)\n",
        "                transition = (current_state, act, reward, next_state, game_over)\n",
        "                # self.test(transition)\n",
        "\n",
        "                self.memory.push(transition)\n",
        "\n",
        "                self.total_step += 1\n",
        "                self.step_since_update += 1\n",
        "\n",
        "                if game_over:\n",
        "                    break\n",
        "\n",
        "                current_state = next_state\n",
        "                act = self.compute_action(current_state)\n",
        "\n",
        "                if t % self.learn_freq != 0: continue\n",
        "\n",
        "                states, actions, rewards, next_states, not_done_mask = self.memory.get_batch(self.batch_size)\n",
        "                # image1=Image.fromarray(states[0])\n",
        "                # image1.show('1')\n",
        "                # image2 = Image.fromarray(next_states[0])\n",
        "                # image2.show('2')\n",
        "                # print(actions[0],rewards[0],not_done_mask[0])\n",
        "                # input()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    Q_t_plus_one_max = self.target_network(next_states).max(1)[0]\n",
        "                    Q_t_plus_one = Q_t_plus_one_max * not_done_mask\n",
        "                    Q_target = rewards + self.gamma * Q_t_plus_one\n",
        "\n",
        "                self.network.train()\n",
        "                Q_t = self.network(states)\n",
        "                Q_t = Q_t.gather(1, actions).squeeze()\n",
        "\n",
        "                assert Q_t.shape == Q_target.shape, print(Q_t.shape, Q_target.shape)\n",
        "\n",
        "                # Update the network\n",
        "                self.optimizer.zero_grad()\n",
        "                loss = self.loss(input=Q_t, target=Q_target)\n",
        "                loss_value = loss.item()\n",
        "                stat['loss'].append(loss_value)\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.network.parameters(), self.clip_norm)\n",
        "                self.optimizer.step()\n",
        "                self.network.eval()\n",
        "\n",
        "            # update target network\n",
        "            if self.step_since_update > self.target_update_freq:\n",
        "                if train_iteration % 10000 == 0:\n",
        "                  print('\\nCurrent train iteration:{} Current memory:{}'.format(train_iteration,len(self.memory)))\n",
        "                  print('Current step:{},{} steps has passed since last update,Now update behavior policy'.format(self.total_step, self.step_since_update))\n",
        "\n",
        "                self.step_since_update = 0\n",
        "                self.target_network.load_state_dict(self.network.state_dict())\n",
        "                self.target_network.eval()\n",
        "\n",
        "                # evaluate and save network\n",
        "                self.step_since_evaluate += 1\n",
        "                if self.step_since_evaluate >= self.evaluate_int:\n",
        "                    self.step_since_evaluate = 0\n",
        "                    eva_score, eva_length, eva_reward = self.evaluate()\n",
        "                    if train_iteration % 10000 == 0:\n",
        "                      print(\"best score:{},loss:{:.2f},episode length:{},evaluate score:{},evaluate reward:{:.2f}\"\n",
        "                            .format(self.best_scores,np.mean(stat[\"loss\"]),eva_length ,eva_score,eva_reward))\n",
        "\n",
        "                    # save best network\n",
        "                    if eva_score > self.best_scores and eva_score > 1:\n",
        "                        print('save model of performance:', eva_score)\n",
        "                        self.best_scores = eva_score\n",
        "                        torch.save(self.target_network, 'best_{}_{}.pt'.format(eva_score, self.total_step))\n",
        "\n",
        "    def evaluate(self, weights=None, num_episodes=30, episodes_len=100):\n",
        "        #pbar = tqdm.tqdm(total=num_episodes, desc=\"evaluating\")\n",
        "        env = Snake_Env()\n",
        "        policy = self.target_network\n",
        "        if weights:\n",
        "            policy.load_state_dict(weights)\n",
        "        rewards = []\n",
        "        epo_len = []\n",
        "        scores = []\n",
        "        for i in range(num_episodes):\n",
        "            obs = env.reset()\n",
        "            with torch.no_grad():\n",
        "                act = np.argmax(policy(obs).cpu().detach().numpy())\n",
        "            epo = 0\n",
        "            score = 0\n",
        "            ep_reward=0\n",
        "            for t in range(episodes_len):\n",
        "                next_state, reward, score, game_over, other_info = env.frame_step(act)\n",
        "                act = np.argmax(policy(next_state).cpu().detach().numpy())\n",
        "                if game_over:\n",
        "                    break\n",
        "                epo += 1\n",
        "                ep_reward+=reward\n",
        "            epo_len.append(epo)\n",
        "            rewards.append(ep_reward)\n",
        "            scores.append(score)\n",
        "            #pbar.update()\n",
        "        #pbar.close()\n",
        "        return np.mean(scores), np.mean(epo_len),np.mean(rewards)\n",
        "\n",
        "\n",
        "def demo():\n",
        "    pass\n",
        "\n",
        "DQN = DQN_Method()\n",
        "#DQN.train(pre_trained='best_3.1_80096.pt')\n",
        "DQN.train()\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current memory contains 500 transitions,start learning!\n",
            "Start Training\n",
            "\n",
            "Current train iteration:0 Current memory:539\n",
            "Current step:34,34 steps has passed since last update,Now update behavior policy\n",
            "best score:0,loss:36.28,episode length:100.0,evaluate score:0.06666666666666667,evaluate reward:9.05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-ee6edef3135c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0mDQN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN_Method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;31m#DQN.train(pre_trained='best_3.1_80096.pt')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m \u001b[0mDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-ee6edef3135c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, pre_trained)\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_since_evaluate\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_int\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_since_evaluate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                     \u001b[0meva_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meva_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meva_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtrain_iteration\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                       print(\"best score:{},loss:{:.2f},episode length:{},evaluate score:{},evaluate reward:{:.2f}\"\n",
            "\u001b[0;32m<ipython-input-8-ee6edef3135c>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, weights, num_episodes, episodes_len)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-2f61df6de9f5>\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mframe_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-2f61df6de9f5>\u001b[0m in \u001b[0;36minit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msurfarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_surface\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-2f61df6de9f5>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# pygame init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SCREEN_WIDTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SCREEN_HEIGHT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_caption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Snake     Score:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}